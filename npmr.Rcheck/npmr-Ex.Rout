
R version 3.3.2 (2016-10-31) -- "Sincere Pumpkin Patch"
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "npmr"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('npmr')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Vowel")
> ### * Vowel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Vowel
> ### Title: Vowel Recognition
> ### Aliases: Vowel
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Vowel)
> summary(Vowel)
      y               subset         x.1              x.2        
 Length:990         test :462   Min.   :-5.211   Min.   :-1.274  
 Class :character   train:528   1st Qu.:-3.888   1st Qu.: 1.052  
 Mode  :character               Median :-3.146   Median : 1.877  
                                Mean   :-3.204   Mean   : 1.882  
                                3rd Qu.:-2.603   3rd Qu.: 2.738  
                                Max.   :-0.941   Max.   : 5.074  
      x.3                x.4               x.5               x.6         
 Min.   :-2.48700   Min.   :-1.4090   Min.   :-2.1270   Min.   :-0.8360  
 1st Qu.:-0.97575   1st Qu.:-0.0655   1st Qu.:-0.7690   1st Qu.: 0.1960  
 Median :-0.57250   Median : 0.4335   Median :-0.2990   Median : 0.5520  
 Mean   :-0.50777   Mean   : 0.5155   Mean   :-0.3057   Mean   : 0.6302  
 3rd Qu.:-0.06875   3rd Qu.: 1.0960   3rd Qu.: 0.1695   3rd Qu.: 1.0285  
 Max.   : 1.43100   Max.   : 2.3770   Max.   : 1.8310   Max.   : 2.3270  
      x.7                 x.8                x.9                x.10         
 Min.   :-1.537000   Min.   :-1.29300   Min.   :-1.61300   Min.   :-1.68000  
 1st Qu.:-0.307000   1st Qu.:-0.09575   1st Qu.:-0.70400   1st Qu.:-0.54800  
 Median : 0.022000   Median : 0.32800   Median :-0.30250   Median :-0.15650  
 Mean   :-0.004365   Mean   : 0.33655   Mean   :-0.30298   Mean   :-0.07134  
 3rd Qu.: 0.296500   3rd Qu.: 0.77000   3rd Qu.: 0.09375   3rd Qu.: 0.37100  
 Max.   : 1.403000   Max.   : 2.03900   Max.   : 1.30900   Max.   : 1.39600  
> 
> 
> 
> cleanEx()
> nameEx("cv.npmr")
> ### * cv.npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.npmr
> ### Title: Cross-validated nuclear penalized multinomial regression
> ### Aliases: cv.npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> fold = sample(rep(1:10, length = nrow(X)))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)
102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100100> 
> # Print the NPMR fit:
> fit2

 Call:
cv.npmr(X = X, Y = Y, lambda = exp(seq(7, -2)), foldid = fold)

Optimal lambda: 7.389056 
Rank of fitted coefficient matrix: 9 
Mean CV error: 1.999115 

> 
> # Produce a biplot:
> plot(fit2)
> 
> # Compute mean test error using the predict function:
> -mean(log(rowSums(Ytest*predict(fit2, Xtest))))
[1] 1.969769
> 
> 
> 
> cleanEx()
> nameEx("npmr-package")
> ### * npmr-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: npmr-package
> ### Title: Nuclear penalized multinomial regression
> ### Aliases: npmr-package
> ### Keywords: package
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))
102030405060708090100> 
> # Print the NPMR fit:
> fit2

 Call:
npmr(X = X, Y = Y, lambda = exp(seq(7, -2)))

         lambda rank objective
1  1096.6331584    0  2299.695
2   403.4287935    0  2299.695
3   148.4131591    0  2299.695
4    54.5981500    9  2227.284
5    20.0855369    9  2056.834
6     7.3890561    9  1961.523
7     2.7182818    9  1921.425
8     1.0000000    9  1905.945
9     0.3678794    9  1900.152
10    0.1353353    9  1898.010

> 
> # Produce a biplot:
> plot(fit2, lambda = 20)
> 
> # Compute mean test error using the predict function (for each value of lambda):
> getloss = function(pred, Y) {
+     -mean(log(rowSums(Y*pred)))
+ }
> apply(predict(fit2, Xtest), 3, getloss, Ytest)
 [1] 2.305113 2.305113 2.305113 2.100656 1.985149 1.972045 1.972295 1.973167
 [9] 1.973570 1.973864
> 
> 
> 
> cleanEx()
> nameEx("npmr")
> ### * npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: npmr
> ### Title: Nuclear penalized multinomial regression
> ### Aliases: npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))
102030405060708090100> 
> # Print the NPMR fit:
> fit2

 Call:
npmr(X = X, Y = Y, lambda = exp(seq(7, -2)))

         lambda rank objective
1  1096.6331584    0  2299.695
2   403.4287935    0  2299.695
3   148.4131591    0  2299.695
4    54.5981500    9  2227.284
5    20.0855369    9  2056.834
6     7.3890561    9  1961.523
7     2.7182818    9  1921.425
8     1.0000000    9  1905.945
9     0.3678794    9  1900.152
10    0.1353353    9  1898.010

> 
> # Produce a biplot:
> plot(fit2, lambda = 20)
> 
> # Compute mean test error using the predict function (for each value of lambda):
> getloss = function(pred, Y) {
+     -mean(log(rowSums(Y*pred)))
+ }
> apply(predict(fit2, Xtest), 3, getloss, Ytest)
 [1] 2.305113 2.305113 2.305113 2.100656 1.985149 1.972045 1.972295 1.973167
 [9] 1.973570 1.973864
> 
> 
> 
> cleanEx()
> nameEx("plot.cv.npmr")
> ### * plot.cv.npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.cv.npmr
> ### Title: Visualize the regression coefficient matrix fit by
> ###   cross-validated NPMR
> ### Aliases: plot.cv.npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> fold = sample(rep(1:10, length = nrow(X)))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)
102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100100> 
> # Produce a biplot:
> plot(fit2)
> 
> 
> 
> cleanEx()
> nameEx("plot.npmr")
> ### * plot.npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.npmr
> ### Title: Visualize the regression coefficient matrix fit by
> ###   cross-validated NPMR
> ### Aliases: plot.npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))
102030405060708090100> 
> # Produce a biplot:
> plot(fit2, lambda = 20)
> 
> 
> 
> cleanEx()
> nameEx("predict.cv.npmr")
> ### * predict.cv.npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.cv.npmr
> ### Title: Make predictions from a "cv.npmr" object
> ### Aliases: predict.cv.npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> fold = sample(rep(1:10, length = nrow(X)))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)
102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100100> 
> # Compute mean test error using the predict function:
> -mean(log(rowSums(Ytest*predict(fit2, Xtest))))
[1] 1.969769
> 
> 
> 
> cleanEx()
> nameEx("predict.npmr")
> ### * predict.npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.npmr
> ### Title: Make predictions from a "npmr" object
> ### Aliases: predict.npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))
102030405060708090100> 
> # Compute mean test error using the predict function (for each value of lambda):
> getloss = function(pred, Y) {
+     -mean(log(rowSums(Y*pred)))
+ }
> apply(predict(fit2, Xtest), 3, getloss, Ytest)
 [1] 2.305113 2.305113 2.305113 2.100656 1.985149 1.972045 1.972295 1.973167
 [9] 1.973570 1.973864
> 
> 
> 
> cleanEx()
> nameEx("print.cv.npmr")
> ### * print.cv.npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.cv.npmr
> ### Title: summarize a "cv.npmr" object
> ### Aliases: print.cv.npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> fold = sample(rep(1:10, length = nrow(X)))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = cv.npmr(X, Y, lambda = exp(seq(7, -2)), foldid = fold)
102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100102030405060708090100100> 
> # Print the NPMR fit:
> fit2

 Call:
cv.npmr(X = X, Y = Y, lambda = exp(seq(7, -2)), foldid = fold)

Optimal lambda: 7.389056 
Rank of fitted coefficient matrix: 9 
Mean CV error: 1.999115 

> 
> 
> 
> cleanEx()
> nameEx("print.npmr")
> ### * print.npmr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.npmr
> ### Title: Summarize a "npmr" object
> ### Aliases: print.npmr
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> #   Fit NPMR to simulated data
> 
> K = 5
> n = 1000
> m = 10000
> p = 10
> r = 2
> 
> # Simulated training data
> set.seed(8369)
> A = matrix(rnorm(p*r), p, r)
> C = matrix(rnorm(K*r), K, r)
> B = tcrossprod(A, C)            # low-rank coefficient matrix
> X = matrix(rnorm(n*p), n, p)    # covariate matrix with iid Gaussian entries
> eta = X 
> P = exp(eta)/rowSums(exp(eta))
> Y = t(apply(P, 1, rmultinom, n = 1, size = 1))
> 
> # Simulate test data
> Xtest = matrix(rnorm(m*p), m, p)
> etatest = Xtest 
> Ptest = exp(etatest)/rowSums(exp(etatest))
> Ytest = t(apply(Ptest, 1, rmultinom, n = 1, size = 1))
> 
> # Fit NPMR for a sequence of lambda values without CV:
> fit2 = npmr(X, Y, lambda = exp(seq(7, -2)))
102030405060708090100> 
> # Print the NPMR fit:
> fit2

 Call:
npmr(X = X, Y = Y, lambda = exp(seq(7, -2)))

         lambda rank objective
1  1096.6331584    0  2299.695
2   403.4287935    0  2299.695
3   148.4131591    0  2299.695
4    54.5981500    9  2227.284
5    20.0855369    9  2056.834
6     7.3890561    9  1961.523
7     2.7182818    9  1921.425
8     1.0000000    9  1905.945
9     0.3678794    9  1900.152
10    0.1353353    9  1898.010

> 
> 
> 
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  4.998 0.326 5.395 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
